# E-Invoicing Web Scraper Dashboard

A fully functional web scraping dashboard that monitors and extracts e-invoicing related content from [BOSA Belgium](https://bosa.belgium.be/en), stores data in Excel format, tracks new publications, and provides analytics visualization.

## Features

### ğŸ” Web Scraping
- Automatically scrapes e-invoicing content from BOSA Belgium website
- Searches for keywords: e-invoicing, electronic invoicing, e-billing, digital invoicing, PEPPOL
- Handles pagination and multiple pages
- Extracts publication dates, URLs, titles, and content summaries

### ğŸ“Š Data Management
- Stores data in Excel format (.xlsx)
- Automatic duplicate detection based on URL
- Tracks new vs existing posts
- Maintains scrape timestamps
- Data persists across sessions using window.storage API

### ğŸ“ˆ Analytics Dashboard
- Total posts counter
- New posts indicator
- Publication frequency charts (monthly)
- Recent posts list with search and filtering
- Date range filtering
- Export functionality

### ğŸ¨ User Interface
- Clean, modern, responsive design
- Real-time status updates
- Interactive post details modal
- Direct links to original posts
- Error handling with user-friendly messages

## Installation

1. Install dependencies:
```bash
npm install
```

2. Install Python dependencies:
```bash
pip3 install -r requirements.txt
```

3. Start both servers (backend proxy + frontend):
```bash
npm run dev:all
```

Or start them separately:
```bash
# Terminal 1: Backend proxy server (Python Flask on port 3002)
npm run server
# or: python3 server.py

# Terminal 2: Frontend dev server (port 3000)
npm run dev
```

4. Open your browser to `http://localhost:3000`

**Important**: The backend proxy server (Python Flask on port 3002) is required to bypass CORS restrictions. Without it, scraping will fail due to browser security policies.

## Usage

1. **Initial Setup**: When you first open the dashboard, click "Scrape Now" to start collecting data.

2. **Scraping**: Click the "Scrape Now" button to fetch new e-invoicing content from the BOSA website.

3. **Viewing Data**: 
   - Browse recent posts in the main list
   - Click on any post to view full details
   - Use search and date filters to find specific content

4. **Exporting**: 
   - Click "Download Excel" to download all data
   - Use "Export Filtered" to download filtered results

5. **Analytics**: View publication frequency charts and statistics in the dashboard.

## Technical Details

### Storage
- Uses `window.storage` API (with localStorage fallback)
- Data stored as base64-encoded Excel files
- Also maintains JSON format for quick access

### CORS Handling
The application includes a **Python Flask backend proxy server** that handles all web scraping requests server-side, completely bypassing CORS restrictions.

**Architecture:**
- Frontend (React) runs on port 3000
- Backend proxy (Python Flask) runs on port 3002
- All scraping requests go through the backend proxy
- No CORS issues since server-to-server requests don't have CORS restrictions

**Python Dependencies:**
- Flask - Web framework
- flask-cors - CORS handling
- requests - HTTP client for scraping

**Fallback Options:**
- If backend proxy is unavailable, tries direct fetch
- Falls back to external CORS proxies as last resort

### Excel Structure
- Column A: Publication Date
- Column B: Post URL
- Column C: Post Title
- Column D: Content Summary/Excerpt
- Column E: Scrape Timestamp
- Column F: Status (New/Existing)

## Dependencies

- **React** - UI framework
- **SheetJS (xlsx)** - Excel file handling
- **Recharts** - Data visualization
- **Vite** - Build tool and dev server

## Project Structure

```
belgium-scraper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.jsx          # Main dashboard component
â”‚   â”œâ”€â”€ App.css          # Styles
â”‚   â”œâ”€â”€ main.jsx         # React entry point
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ scraper.js   # Web scraping logic
â”‚       â””â”€â”€ excelHandler.js  # Excel file operations
â”œâ”€â”€ server.py            # Python Flask backend proxy server
â”œâ”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ index.html
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## Error Handling

The application handles:
- Website unavailability
- Empty search results
- Network errors
- CORS restrictions
- Storage quota limits
- Malformed HTML

All errors are displayed to users with clear messages.

## Future Enhancements

- Email notifications for new posts
- Scheduled automatic scraping
- Advanced keyword highlighting
- Content categorization
- Multiple export formats
- Backend proxy integration

## License

MIT License

## Support

For issues or questions, please check the browser console for detailed error messages. The scraper logs all activities for debugging purposes.

